"""
NLP Signals Extractor - Sentiment from News & Filings

Extracts sentiment and entity-specific signals from:
- News articles (Bloomberg, Reuters)
- SEC filings (10-K, 10-Q, 8-K)
- Earnings call transcripts

Model: FinBERT (ProsusAI/finbert)
Evaluation Dataset: FiQA Sentiment Analysis (1,173 labeled samples)

SageMaker Integration:
- Model caching in S3 (fast startup)
- Automatic download from HuggingFace if not in S3
- Fallback to local cache

Early Warning Triggers:
- News sentiment < -0.3 (persistent negative)
- Mentions of "covenant breach", "restructuring", "default"
- Management turnover (CFO, CEO changes)

Usage:
    # Local mode
    extractor = NLPSignalExtractor(mode="local")
    
    # SageMaker mode (with S3 caching)
    extractor = NLPSignalExtractor(
        mode="sagemaker",
        s3_bucket="my-bucket",
        s3_model_prefix="models/finbert/"
    )
    
    documents = [
        "ABC Corp announced disappointing Q3 results...",
        "CFO Jane Doe to step down amid restructuring...",
    ]
    
    signals = extractor.extract_signals(documents)
"""

from typing import Dict, List, Optional, Literal
import numpy as np
import re
import os
from pathlib import Path

try:
    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers not available")

try:
    import boto3
    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False
    print("Warning: boto3 not available (S3 disabled)")


class NLPSignalExtractor:
    """
    Extract NLP-based credit signals from text
    
    Uses FinBERT for financial sentiment analysis.
    Evaluation: FiQA Sentiment dataset (87% F1)
    
    SageMaker Features:
    - S3 model caching (fast startup)
    - Automatic model download
    - Graceful fallbacks
    """
    
    def __init__(
        self,
        model_name: str = "ProsusAI/finbert",
        mode: Literal["local", "sagemaker", "production"] = "local",
        s3_bucket: Optional[str] = None,
        s3_model_prefix: str = "models/finbert/",
        cache_dir: str = "models/cache/finbert"
    ):
        """
        Initialize NLP signal extractor
        
        Args:
            model_name: HuggingFace model name
            mode: Execution mode (local/sagemaker/production)
            s3_bucket: S3 bucket for model caching (SageMaker mode)
            s3_model_prefix: S3 prefix for model files
            cache_dir: Local cache directory
        """
        self.model_name = model_name
        self.mode = mode
        self.s3_bucket = s3_bucket
        self.s3_model_prefix = s3_model_prefix
        self.cache_dir = Path(cache_dir)
        
        # Initialize S3 client if available
        if mode == "sagemaker" and BOTO3_AVAILABLE and s3_bucket:
            self.s3_client = boto3.client('s3')
        else:
            self.s3_client = None
        
        # Load model
        self.sentiment_model = self._load_model()
        
        # Negative keywords (covenant breach, restructuring, etc.)
        self.negative_keywords = [
            "covenant breach", "default", "restructuring", "bankruptcy",
            "liquidity crisis", "going concern", "debt restructuring",
            "covenant violation", "financial distress", "insolvency",
            "chapter 11", "creditor protection", "workout", "forbearance"
        ]
        
        # Management turnover keywords
        self.turnover_keywords = [
            "ceo resign", "cfo resign", "cfo depart", "ceo step down",
            "chief financial officer resign", "chief executive officer resign",
            "management change", "leadership transition"
        ]
    
    def _load_model(self):
        """Load FinBERT model with S3 caching"""
        if not TRANSFORMERS_AVAILABLE:
            print("âš  Transformers not available, using rule-based fallback")
            return None
        
        # Step 1: Check if model in local cache
        if self._model_in_local_cache():
            print(f"âœ“ Loading FinBERT from local cache: {self.cache_dir}")
            return self._load_from_local_cache()
        
        # Step 2: Check if model in S3 (SageMaker mode)
        if self.mode == "sagemaker" and self.s3_client:
            if self._model_in_s3():
                print(f"âœ“ Downloading FinBERT from S3: s3://{self.s3_bucket}/{self.s3_model_prefix}")
                self._download_model_from_s3()
                return self._load_from_local_cache()
        
        # Step 3: Download from HuggingFace
        print(f"âš  Downloading FinBERT from HuggingFace: {self.model_name}")
        model = self._download_from_huggingface()
        
        # Step 4: Cache to S3 for future use (SageMaker mode)
        if self.mode == "sagemaker" and self.s3_client:
            print(f"âœ“ Uploading FinBERT to S3 for future use")
            self._upload_model_to_s3()
        
        return model
    
    def _model_in_local_cache(self) -> bool:
        """Check if model exists in local cache"""
        return (self.cache_dir / "config.json").exists()
    
    def _model_in_s3(self) -> bool:
        """Check if model exists in S3"""
        if not self.s3_client:
            return False
        
        try:
            self.s3_client.head_object(
                Bucket=self.s3_bucket,
                Key=f"{self.s3_model_prefix}config.json"
            )
            return True
        except:
            return False
    
    def _load_from_local_cache(self):
        """Load model from local cache"""
        try:
            model = pipeline(
                "sentiment-analysis",
                model=str(self.cache_dir),
                tokenizer=str(self.cache_dir),
                truncation=True,
                max_length=512
            )
            return model
        except Exception as e:
            print(f"âš  Error loading from cache: {e}")
            return None
    
    def _download_from_huggingface(self):
        """Download model from HuggingFace and cache locally"""
        try:
            # Create cache directory
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            
            # Download model
            model = pipeline(
                "sentiment-analysis",
                model=self.model_name,
                truncation=True,
                max_length=512
            )
            
            # Save to cache
            model.model.save_pretrained(str(self.cache_dir))
            model.tokenizer.save_pretrained(str(self.cache_dir))
            
            print(f"âœ“ Cached FinBERT locally: {self.cache_dir}")
            
            return model
        except Exception as e:
            print(f"âš  Error downloading from HuggingFace: {e}")
            return None
    
    def _download_model_from_s3(self):
        """Download model from S3 to local cache"""
        try:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            
            # List all model files in S3
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_bucket,
                Prefix=self.s3_model_prefix
            )
            
            if 'Contents' not in response:
                print(f"âš  No files found in S3: s3://{self.s3_bucket}/{self.s3_model_prefix}")
                return
            
            # Download each file
            for obj in response['Contents']:
                s3_key = obj['Key']
                local_file = self.cache_dir / s3_key.replace(self.s3_model_prefix, '')
                
                # Create subdirectories if needed
                local_file.parent.mkdir(parents=True, exist_ok=True)
                
                # Download
                self.s3_client.download_file(
                    self.s3_bucket,
                    s3_key,
                    str(local_file)
                )
            
            print(f"âœ“ Downloaded FinBERT from S3 to {self.cache_dir}")
        
        except Exception as e:
            print(f"âš  Error downloading from S3: {e}")
    
    def _upload_model_to_s3(self):
        """Upload model from local cache to S3"""
        try:
            # Upload all files in cache directory
            for local_file in self.cache_dir.rglob('*'):
                if local_file.is_file():
                    s3_key = f"{self.s3_model_prefix}{local_file.relative_to(self.cache_dir)}"
                    
                    self.s3_client.upload_file(
                        str(local_file),
                        self.s3_bucket,
                        s3_key
                    )
            
            print(f"âœ“ Uploaded FinBERT to s3://{self.s3_bucket}/{self.s3_model_prefix}")
        
        except Exception as e:
            print(f"âš  Error uploading to S3: {e}")
    
    def extract_signals(
        self,
        documents: List[str],
        recency_decay: float = 0.1
    ) -> Dict:
        """
        Extract NLP signals from documents
        
        Args:
            documents: List of text documents (news, filings, etc.)
            recency_decay: Decay factor for older documents
            
        Returns:
            {
                "news_sentiment": {"score": float, "trend": str},
                "entities": List[Dict],
                "warning_flags": List[str],
            }
        """
        if not documents:
            return {
                "news_sentiment": {"score": 0.0, "trend": "stable"},
                "entities": [],
                "warning_flags": [],
            }
        
        # 1. Aggregate sentiment
        sentiments = []
        for doc in documents:
            sentiment = self._get_sentiment(doc)
            sentiments.append(sentiment)
        
        # Recency-weighted sentiment
        weights = np.exp(-np.arange(len(sentiments)) * recency_decay)
        weighted_sentiment = np.average(
            [s["score"] if s["label"] == "positive" else -s["score"] for s in sentiments],
            weights=weights
        )
        
        # Trend determination
        if weighted_sentiment < -0.2:
            trend = "deteriorating"
        elif weighted_sentiment > 0.2:
            trend = "improving"
        else:
            trend = "stable"
        
        # 2. Extract entities (negative events)
        entities = self._extract_entities(documents)
        
        # 3. Warning flags
        warning_flags = self._detect_warning_flags(documents)
        
        return {
            "news_sentiment": {
                "score": weighted_sentiment,
                "trend": trend,
                "sample_size": len(documents),
            },
            "entities": entities,
            "warning_flags": warning_flags,
        }
    
    def _get_sentiment(self, text: str) -> Dict:
        """
        Get sentiment for single text
        
        Args:
            text: Text to analyze
            
        Returns:
            {"label": "positive/negative/neutral", "score": 0.0-1.0}
        """
        if self.sentiment_model is None:
            # Fallback: rule-based
            return self._rule_based_sentiment(text)
        
        try:
            # Truncate to 512 tokens
            result = self.sentiment_model(text[:2000])[0]
            return result
        except Exception as e:
            # Fallback
            return self._rule_based_sentiment(text)
    
    def _rule_based_sentiment(self, text: str) -> Dict:
        """Fallback rule-based sentiment"""
        text_lower = text.lower()
        
        # Simple keyword matching
        positive_words = ["growth", "strong", "exceed", "beat", "outperform", "success"]
        negative_words = ["decline", "miss", "weak", "disappointing", "concern", "risk"]
        
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)
        
        if neg_count > pos_count:
            return {"label": "negative", "score": 0.7}
        elif pos_count > neg_count:
            return {"label": "positive", "score": 0.7}
        else:
            return {"label": "neutral", "score": 0.5}
    
    def _extract_entities(self, documents: List[str]) -> List[Dict]:
        """
        Extract credit-relevant entities
        
        Focus on:
        - Covenant breaches
        - Management changes
        - Downgrades
        """
        entities = []
        
        for doc in documents:
            doc_lower = doc.lower()
            
            # Check for covenant breach
            for keyword in self.negative_keywords:
                if keyword in doc_lower:
                    entities.append({
                        "type": "negative_event",
                        "keyword": keyword,
                        "context": self._extract_context(doc, keyword),
                    })
            
            # Check for management turnover
            for keyword in self.turnover_keywords:
                if keyword in doc_lower:
                    entities.append({
                        "type": "management_turnover",
                        "keyword": keyword,
                        "context": self._extract_context(doc, keyword),
                    })
        
        return entities
    
    def _detect_warning_flags(self, documents: List[str]) -> List[str]:
        """Detect high-severity warning flags"""
        flags = []
        
        combined_text = " ".join(documents).lower()
        
        # Critical keywords
        if any(kw in combined_text for kw in ["covenant breach", "default", "bankruptcy"]):
            flags.append("CRITICAL: Covenant breach or default mentioned")
        
        if any(kw in combined_text for kw in ["cfo resign", "ceo resign"]):
            flags.append("HIGH: Management turnover detected")
        
        if "going concern" in combined_text:
            flags.append("CRITICAL: Going concern warning")
        
        if "liquidity crisis" in combined_text or "cash flow crisis" in combined_text:
            flags.append("HIGH: Liquidity concerns")
        
        return flags
    
    def _extract_context(self, text: str, keyword: str, window: int = 100) -> str:
        """Extract context around keyword"""
        text_lower = text.lower()
        keyword_lower = keyword.lower()
        
        pos = text_lower.find(keyword_lower)
        if pos == -1:
            return ""
        
        start = max(0, pos - window)
        end = min(len(text), pos + len(keyword) + window)
        
        context = text[start:end]
        return f"...{context}..."


# Example usage
if __name__ == "__main__":
    # Local mode
    print("Testing LOCAL mode:")
    extractor_local = NLPSignalExtractor(mode="local")
    
    # SageMaker mode
    print("\nTesting SAGEMAKER mode:")
    extractor_sagemaker = NLPSignalExtractor(
        mode="sagemaker",
        s3_bucket="my-sagemaker-bucket",
        s3_model_prefix="models/finbert/"
    )
    
    # Sample documents
    documents = [
        "ABC Corp announced disappointing Q3 results with revenue missing analyst expectations by 15%.",
        "CFO Jane Doe to step down effective immediately amid ongoing restructuring efforts.",
        "Moody's downgrades ABC Corp to Ba3 from Baa3, citing deteriorating leverage metrics.",
    ]
    
    signals = extractor_local.extract_signals(documents)
    
    print("\n" + "="*60)
    print("NLP Signals:")
    print("="*60)
    print(f"News Sentiment: {signals['news_sentiment']['score']:.2f}")
    print(f"Trend: {signals['news_sentiment']['trend']}")
    
    print(f"\nEntities Detected: {len(signals['entities'])}")
    for entity in signals['entities']:
        print(f"  â€¢ {entity['type']}: {entity['keyword']}")
    
    print(f"\nWarning Flags: {len(signals['warning_flags'])}")
    for flag in signals['warning_flags']:
        print(f"  ðŸš¨ {flag}")